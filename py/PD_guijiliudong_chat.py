import json
import os
import time as time_module
from datetime import datetime

# å¯é€‰å¯¼å…¥ï¼Œé¿å…å¯¼å…¥é”™è¯¯
try:
    import requests
except ImportError:
    requests = None

try:
    import urllib3
except ImportError:
    urllib3 = None

class PD_guijiliudong_chat:
    """
    ç¡…åŸºæµåŠ¨å¯¹è¯æ¨¡å‹èŠ‚ç‚¹
    ç”¨äºæ™®é€šå¯¹è¯ã€ä»£ç ç”Ÿæˆç­‰ä»»åŠ¡ï¼ˆä¸åŒ…å«æ¨ç†æ¨¡å‹ï¼‰
    """
    
    def __init__(self):
        pass
    
    @classmethod
    def INPUT_TYPES(cls):
        # å¯¹è¯æ¨¡å‹åˆ—è¡¨ï¼ˆæ’é™¤æ¨ç†æ¨¡å‹ï¼‰
        chat_models = [
            # ğŸ”¥ æ¨èæ¨¡å‹
            "Qwen/Qwen2.5-72B-Instruct",
            "Qwen/Qwen2.5-32B-Instruct",
            "Qwen/Qwen2.5-14B-Instruct",
            "Qwen/Qwen2.5-7B-Instruct",
            "deepseek-ai/DeepSeek-V2.5",
            "THUDM/glm-4-9b-chat",
            
            # DeepSeekç³»åˆ—ï¼ˆå¯¹è¯ï¼‰
            "deepseek-ai/DeepSeek-V3.2-Exp",
            "Pro/deepseek-ai/DeepSeek-V3.2-Exp",
            "deepseek-ai/DeepSeek-V3",
            "Pro/deepseek-ai/DeepSeek-V3",
            "deepseek-ai/DeepSeek-Coder-V2-Instruct",
            
            # Qwenç³»åˆ—
            "Qwen/Qwen3-Next-80B-A3B-Instruct",
            "Qwen/Qwen3-32B",
            "Qwen/Qwen3-14B",
            "Qwen/Qwen3-8B",
            "Qwen/Qwen3-30B-A3B-Instruct-2507",
            "Qwen/Qwen3-235B-A22B-Instruct-2507",
            "Qwen/Qwen2.5-72B-Instruct-128K",
            "Qwen/Qwen2.5-Coder-32B-Instruct",
            "Qwen/Qwen2.5-Coder-7B-Instruct",
            "Qwen/Qwen2-7B-Instruct",
            "Pro/Qwen/Qwen2.5-7B-Instruct",
            "Pro/Qwen/Qwen2-7B-Instruct",
            "Tongyi-Zhiwen/QwenLong-L1-32B",
            
            # GLMç³»åˆ—
            "zai-org/GLM-4.6",
            "zai-org/GLM-4.5-Air",
            "zai-org/GLM-4.5",
            "THUDM/GLM-Z1-32B-0414",
            "THUDM/GLM-4-32B-0414",
            "THUDM/GLM-Z1-Rumination-32B-0414",
            "THUDM/GLM-4-9B-0414",
            "Pro/THUDM/glm-4-9b-chat",
            
            # å…¶ä»–æ¨¡å‹
            "inclusionAI/Ling-1T",
            "inclusionAI/Ring-flash-2.0",
            "inclusionAI/Ling-flash-2.0",
            "inclusionAI/Ling-mini-2.0",
            "moonshotai/Kimi-K2-Instruct-0905",
            "ByteDance-Seed/Seed-OSS-36B-Instruct",
            "stepfun-ai/step3",
            "baidu/ERNIE-4.5-300B-A47B",
            "ascend-tribe/pangu-pro-moe",
            "tencent/Hunyuan-A13B-Instruct",
            "MiniMaxAI/MiniMax-M1-80k",
            "internlm/internlm2_5-7b-chat"
        ]
        
        return {
            "required": {
                "system_prompt": ("STRING", {
                    "default": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜æä¾›å‡†ç¡®ã€æœ‰å¸®åŠ©çš„å›ç­”ã€‚",
                    "multiline": True
                }),
            },
            "optional": {
                "user_prompt": ("STRING", {
                    "default": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
                    "multiline": True
                }),
                "model": (chat_models, {"default": "Qwen/Qwen2.5-32B-Instruct"}),
                "api_key": ("STRING", {"default": "", "multiline": False}),
                "temperature": ("FLOAT", {"default": 0.7, "min": 0.0, "max": 2.0, "step": 0.1}),
                "max_tokens": ("INT", {"default": 2048, "min": 1, "max": 8192}),
                "top_p": ("FLOAT", {"default": 0.9, "min": 0.0, "max": 1.0, "step": 0.1}),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "STRING")
    RETURN_NAMES = ("output_text", "model_name", "info")
    FUNCTION = "generate_chat"
    CATEGORY = "PD_Nodes/Chat"
    
    def load_config(self):
        """ä»config.jsonåŠ è½½APIå¯†é’¥"""
        config_path = os.path.join(os.path.dirname(__file__), "config.json")
        
        if not os.path.exists(config_path):
            raise Exception("æ‰¾ä¸åˆ°config.jsoné…ç½®æ–‡ä»¶")
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                
            api_key = None
            if 'siliconflow_api_key' in config:
                api_key = config['siliconflow_api_key']
            elif 'api_key' in config:
                api_key = config['api_key']
            
            if not api_key or api_key.strip() == "":
                raise Exception("åœ¨config.jsonä¸­æ‰¾ä¸åˆ°æœ‰æ•ˆçš„APIå¯†é’¥")
                
            return api_key.strip()
            
        except Exception as e:
            raise Exception(f"è¯»å–config.jsonå¤±è´¥: {str(e)}")
    
    def calculate_cost(self, model, prompt_tokens, completion_tokens):
        """è®¡ç®—APIè°ƒç”¨è´¹ç”¨ï¼ˆäººæ°‘å¸ï¼‰"""
        # ç¡…åŸºæµåŠ¨ä»·æ ¼è¡¨ï¼ˆå…ƒ/ç™¾ä¸‡tokensï¼‰
        pricing = {
            "Qwen/Qwen2.5-72B-Instruct": {"input": 0.35, "output": 0.35},
            "Qwen/Qwen2.5-32B-Instruct": {"input": 0.14, "output": 0.14},
            "Qwen/Qwen2.5-7B-Instruct": {"input": 0.035, "output": 0.035},
            "deepseek-ai/DeepSeek-V2.5": {"input": 0.14, "output": 0.28},
            "deepseek-ai/DeepSeek-V3": {"input": 0.27, "output": 1.1},
            "THUDM/glm-4-9b-chat": {"input": 0.05, "output": 0.05},
            "deepseek-ai/DeepSeek-Coder-V2-Instruct": {"input": 0.14, "output": 0.28},
        }
        
        # è·å–ä»·æ ¼ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤ä»·æ ¼
        price = pricing.get(model, {"input": 0.1, "output": 0.1})
        
        # è®¡ç®—è´¹ç”¨ï¼ˆå…ƒï¼‰
        input_cost = (prompt_tokens / 1000000) * price["input"]
        output_cost = (completion_tokens / 1000000) * price["output"]
        total_cost = input_cost + output_cost
        
        return {
            "input_cost": input_cost,
            "output_cost": output_cost,
            "total_cost": total_cost
        }
    
    def call_api(self, api_key, model, messages, temperature, max_tokens, top_p):
        """è°ƒç”¨ç¡…åŸºæµåŠ¨API"""
        url = "https://api.siliconflow.cn/v1/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
        }
        
        payload = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "top_p": top_p,
            "stream": False
        }
        
        if not requests:
            raise Exception("requestsåº“æœªå®‰è£…")
        
        response = requests.post(url, headers=headers, json=payload, timeout=120)
        
        if response.status_code == 400:
            try:
                error_detail = response.json().get("error", {}).get("message", "è¯·æ±‚å‚æ•°é”™è¯¯")
            except:
                error_detail = "è¯·æ±‚å‚æ•°é”™è¯¯"
            raise Exception(f"HTTP 400: {error_detail}\næç¤ºï¼šæŸäº›å‚æ•°å¯èƒ½ä¸è¢«è¯¥æ¨¡å‹æ”¯æŒ")
        
        if response.status_code == 403:
            raise Exception("HTTP 403: æƒé™è¢«æ‹’ç»\nå¯èƒ½åŸå› ï¼šProæ¨¡å‹éœ€è¦ä»˜è´¹è®¢é˜…ï¼Œæˆ–å…è´¹é¢åº¦ç”¨å®Œ")
        
        if response.status_code != 200:
            error_msg = f"HTTP {response.status_code}"
            try:
                error_detail = response.json().get("error", {}).get("message", "")
                if error_detail:
                    error_msg += f": {error_detail}"
            except:
                pass
            raise Exception(error_msg)
        
        return response.json()
    
    def generate_chat(self, system_prompt, user_prompt="", model="", api_key="", temperature=0.7, max_tokens=2048, top_p=0.9):
        """ç”Ÿæˆå¯¹è¯å†…å®¹"""
        
        start_time = time_module.time()
        
        try:
            # éªŒè¯è¾“å…¥
            if not user_prompt or not user_prompt.strip():
                raise Exception("user_promptä¸èƒ½ä¸ºç©º")
            
            # è®¾ç½®é»˜è®¤æ¨¡å‹
            if not model or model.strip() == "":
                model = "Qwen/Qwen2.5-32B-Instruct"
            
            # è·å–APIå¯†é’¥
            if not api_key or api_key.strip() == "":
                api_key = self.load_config()
            else:
                api_key = api_key.strip()
            
            # æ„å»ºæ¶ˆæ¯
            messages = []
            if system_prompt and system_prompt.strip():
                messages.append({"role": "system", "content": system_prompt.strip()})
            messages.append({"role": "user", "content": user_prompt.strip()})
            
            # è°ƒç”¨API
            print(f"ğŸ’¬ è°ƒç”¨å¯¹è¯æ¨¡å‹: {model}")
            response = self.call_api(api_key, model, messages, temperature, max_tokens, top_p)
            
            # è®¡ç®—è€—æ—¶
            elapsed_time = time_module.time() - start_time
            
            # æå–ç»“æœ
            if "choices" in response and len(response["choices"]) > 0:
                choice = response["choices"][0]
                message = choice["message"]
                
                generated_text = message.get("content", "")
                
                # æ„å»ºinfoä¿¡æ¯
                info_lines = []
                info_lines.append(f"{'='*40}")
                info_lines.append(f"ğŸ’¬ å¯¹è¯æ¨¡å‹è°ƒç”¨ä¿¡æ¯")
                info_lines.append(f"{'='*40}")
                info_lines.append(f"")
                info_lines.append(f"ğŸ“‹ æ¨¡å‹: {model}")
                info_lines.append(f"â° ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                info_lines.append(f"â±ï¸  è€—æ—¶: {elapsed_time:.2f} ç§’")
                info_lines.append(f"")
                
                # Tokenç»Ÿè®¡å’Œè´¹ç”¨
                if "usage" in response:
                    usage = response["usage"]
                    prompt_tokens = usage.get('prompt_tokens', 0)
                    completion_tokens = usage.get('completion_tokens', 0)
                    total_tokens = usage.get('total_tokens', 0)
                    
                    info_lines.append(f"ğŸ“Š Tokenä½¿ç”¨:")
                    info_lines.append(f"   â€¢ è¾“å…¥: {prompt_tokens:,} tokens")
                    info_lines.append(f"   â€¢ è¾“å‡º: {completion_tokens:,} tokens")
                    info_lines.append(f"   â€¢ æ€»è®¡: {total_tokens:,} tokens")
                    info_lines.append(f"")
                    
                    # è®¡ç®—è´¹ç”¨
                    cost = self.calculate_cost(model, prompt_tokens, completion_tokens)
                    info_lines.append(f"ğŸ’° è´¹ç”¨ä¼°ç®—:")
                    info_lines.append(f"   â€¢ è¾“å…¥: Â¥{cost['input_cost']:.6f}")
                    info_lines.append(f"   â€¢ è¾“å‡º: Â¥{cost['output_cost']:.6f}")
                    info_lines.append(f"   â€¢ æ€»è®¡: Â¥{cost['total_cost']:.6f}")
                    info_lines.append(f"")
                
                # è¾“å‡ºç»Ÿè®¡
                output_len = len(generated_text)
                info_lines.append(f"ğŸ“ è¾“å‡ºé•¿åº¦: {output_len:,} å­—ç¬¦")
                info_lines.append(f"âœ… çŠ¶æ€: {choice.get('finish_reason', 'unknown')}")
                info_lines.append(f"{'='*40}")
                
                info_text = "\n".join(info_lines)
                
                return (generated_text, model, info_text)
            else:
                error_msg = "APIè¿”å›æ ¼å¼å¼‚å¸¸"
                error_info = f"âŒ é”™è¯¯\nğŸ“‹ æ¨¡å‹: {model}\nğŸš« {error_msg}"
                return (error_msg, model, error_info)
                
        except Exception as e:
            elapsed_time = time_module.time() - start_time
            error_msg = f"å¯¹è¯è¿‡ç¨‹å‡ºç°é”™è¯¯: {str(e)}"
            error_info = f"âŒ é”™è¯¯\nğŸ“‹ æ¨¡å‹: {model}\nâ±ï¸  è€—æ—¶: {elapsed_time:.2f}ç§’\nğŸš« {str(e)}"
            print(f"é”™è¯¯: {error_msg}")
            return (error_msg, model, error_info)


# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "PD_guijiliudong_chat": PD_guijiliudong_chat
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "PD_guijiliudong_chat": "PD å¯¹è¯æ¨¡å‹ (Chat)"
}

__all__ = ['NODE_CLASS_MAPPINGS', 'NODE_DISPLAY_NAME_MAPPINGS']

